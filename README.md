# ğŸš€ Publisher-Consumer Web Scraping Pipeline

A robust, scalable web scraping pipeline using Redis for task queuing and MongoDB for data storage. Built with Python and designed for Windows 11 with Docker support.

## ğŸ“‹ Features

* **Publisher-Consumer Architecture**: Scalable task distribution using Redis
* **Web Scraping**: Intelligent content extraction with retry mechanisms
* **Data Storage**: MongoDB with automatic indexing and duplicate prevention
* **Docker Support**: Easy deployment with docker-compose
* **CLI Interface**: Single entry point for all operations
* **MVC Architecture**: Clean separation of concerns
* **Environment Configuration**: All settings via .env file
* **Graceful Shutdown**: Proper signal handling
* **Comprehensive Logging**: Detailed operation tracking

## ğŸ—ï¸ Architecture
![Publisher-Consumer Flowchart](publisher_consumer_flowchart.png)


## ğŸ› ï¸ Installation

### Prerequisites

* Python 3.8+
* Docker Desktop (for Windows 11)
* Git

### Step 1: Clone and Setup

```cmd
# Clone the repository
git clone <your-repo-url>
cd publisher-consumer-pipeline

# Create virtual environment
python -m venv venv

# Activate virtual environment (Windows)
venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Step 2: Create a `.env` File

> **Important:** `.env` is **not included** in the repository.
> You need to create it manually in the project root.

Create a file named `.env` with the following content:

```env
# Redis Configuration
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_QUEUE_NAME=articles_queue

# MongoDB Configuration
MONGO_USERNAME=admin
MONGO_PASSWORD=password123
MONGO_DATABASE=articles_db
MONGO_PORT=27017

# Scraping Configuration
SCRAPER_TIMEOUT=30
SCRAPER_MAX_RETRIES=3
SCRAPER_DELAY=1.0

# Logging
LOG_LEVEL=INFO
```

Edit credentials or ports as needed for your environment.

### Step 3: Start Docker Services

```cmd
# Start Redis and MongoDB with Docker
docker-compose up -d

# Verify services are running
docker-compose ps
```

### Step 4: Run Publisher & Consumer

Open **two terminals**:

**Terminal 1 â€“ Consumer:**

```cmd
python main.py --mode consumer
```

**Terminal 2 â€“ Publisher:**

```cmd
python main.py --mode publisher --file articles.json
```

> âœ… The consumer will process tasks as the publisher pushes them to Redis.

### Step 5: Stop Services

```cmd
# Stop all Docker services
docker-compose down

# Reset all data (optional)
docker-compose down -v
```

---

## ğŸš€ Usage

### Basic Commands

```cmd
# Test connections
python main.py --mode test

# Publish articles to queue
python main.py --mode publisher --file articles.json

# Start consumer (runs until Ctrl+C)
python main.py --mode consumer

# Run both publisher and consumer
python main.py --mode both --file articles.json

# Enable verbose logging
python main.py --mode consumer --verbose
```

### Custom JSON File

Create your own articles file:

```json
[
  {
    "id": "my_article_1",
    "url": "https://example.com/article1",
    "source": "example.com",
    "category": "news",
    "priority": "high"
  }
]
```

Then publish it:

```cmd
python main.py --mode publisher --file my_articles.json
```

---

## ğŸ“ Project Structure

```
publisher-consumer-pipeline/
â”‚
â”œâ”€â”€ core/                   # Core business logic
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ publisher.py        # Task publishing logic
â”‚   â”œâ”€â”€ consumer.py         # Task processing logic
â”‚   â”œâ”€â”€ redis_handler.py    # Redis queue operations
â”‚   â”œâ”€â”€ db_handler.py       # MongoDB operations
â”‚   â””â”€â”€ scraper.py          # Web scraping logic
â”‚
â”œâ”€â”€ models/                 # Data models
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ article.py          # Article and task models
â”‚
â”œâ”€â”€ config/                 # Configuration
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ settings.py         # Settings management
â”‚
â”œâ”€â”€ utils/                  # Utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ logger.py           # Logging setup
â”‚
â”œâ”€â”€ venv/                   # Virtual environment
â”œâ”€â”€ articles.json           # Sample data
â”œâ”€â”€ test_pipeline.py        # Test runner
â”œâ”€â”€ .env                    # Environment variables
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ docker-compose.yml      # Docker services
â””â”€â”€ README.md               # This file
```

---

## âš™ï¸ Configuration

All settings are in the `.env` file.

* Redis: host, port, DB, queue name
* MongoDB: username, password, database, port
* Scraper: timeout, retries, delay
* Logging level

---

## ğŸ³ Docker Commands

```cmd
# Start services
docker-compose up -d

# View logs
docker-compose logs -f

# Stop services
docker-compose down

# Reset data (removes volumes)
docker-compose down -v
```

---


### Logs and Debugging

```cmd
python main.py --mode consumer --verbose
docker-compose logs mongodb
docker-compose logs redis
```

---


"# Publisher-Consumer-Web-Scraping-Pipeline" 
